{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b02af22-735d-48ee-8f18-2a955e216161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class IntegerLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # Initialize weights as floating-point values\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features, dtype=torch.float32))  # Use normal distribution for initialization\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.mm(x, self.weight.t())\n",
    "    \n",
    "    def quantize_weights(self, target_min=--32, target_max=31):\n",
    "        \"\"\"Quantize the weights to a target range with dynamic scaling.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Find the min and max of the weights\n",
    "            weight_min = self.weight.min()\n",
    "            weight_max = self.weight.max()\n",
    "            \n",
    "            # Dynamically scale the target range based on observed weight distribution\n",
    "            scale_factor = 0.3 # Adjust this factor to control how aggressively the weights are scaled\n",
    "            range_span = (weight_max - weight_min) * scale_factor\n",
    "            target_min_scaled = weight_min - range_span / 2\n",
    "            target_max_scaled = weight_max + range_span / 2\n",
    "            \n",
    "            # Scale the weights to the scaled target range\n",
    "            scale = (target_max_scaled - target_min_scaled) / (weight_max - weight_min)\n",
    "            zero_point = target_min_scaled - weight_min * scale\n",
    "            \n",
    "            # Quantize the weights\n",
    "            quantized_weights = torch.round(self.weight * scale + zero_point)\n",
    "            \n",
    "            # Clip the values to ensure they are within the target range\n",
    "            quantized_weights = torch.clamp(quantized_weights, target_min, target_max)\n",
    "            \n",
    "            # Store the quantized weights back into the model\n",
    "            self.weight.data = quantized_weights\n",
    "    \n",
    "class IntegerNet(nn.Module):\n",
    "    def __init__(self, input_size=784):\n",
    "        super().__init__()\n",
    "        # Four layers of IntegerLinear\n",
    "        self.layer1 = IntegerLinear(input_size, 128)\n",
    "        self.layer2 = IntegerLinear(128, 128)\n",
    "        self.layer3 = IntegerLinear(128, 64)\n",
    "        self.layer4 = IntegerLinear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def quantize_weights(self):\n",
    "        \"\"\"Quantize weights for all layers.\"\"\"\n",
    "        self.layer1.quantize_weights()\n",
    "        self.layer2.quantize_weights()\n",
    "        self.layer3.quantize_weights()\n",
    "        self.layer4.quantize_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbac7b07-ebf2-4ad4-934a-e989818ce91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath='train.csv'):\n",
    "    data = pd.read_csv(filepath)\n",
    "    labels = data['label'].values\n",
    "    pixels = data.drop('label', axis=1).values\n",
    "    \n",
    "    # Convert to binary (0 or 1)\n",
    "    pixels = (pixels > 127).astype(np.float32)\n",
    "    \n",
    "    return torch.FloatTensor(pixels), torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e1971-b174-4b38-a7ef-4324724a1019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc5a6eb-63a3-46b9-9cdf-57ead970f28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, epochs=10, batch_size=128):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "    \n",
    "    n_samples = X_train.shape[0]\n",
    "    n_batches = n_samples // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        \n",
    "        # Shuffle data\n",
    "        indices = torch.randperm(n_samples)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            batch_X = X_train[start_idx:end_idx]\n",
    "            batch_y = y_train[start_idx:end_idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        avg_loss = total_loss / n_batches\n",
    "        accuracy = correct / n_samples\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "        \n",
    "        # Quantize weights after each epoch to reduce their size\n",
    "        model.quantize_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c4a61-290e-4e9a-9a63-671eaff6df4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7500d0-6ba6-4ed2-9d95-bccceba6b928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d1378-edb0-4d73-bdf4-29ca1fd3df02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb20667a-dffa-474b-aa08-be43c7449abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_data()\n",
    "model = IntegerNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "169c555d-485b-4d5a-9570-6f95b1692e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42000, 784])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "03c0c3f3-6fc6-475b-a403-cfd0c19c0a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 236.2272, Accuracy: 0.3083\n",
      "Epoch [2/10], Loss: 11.6330, Accuracy: 0.1227\n",
      "Epoch [3/10], Loss: 14.5360, Accuracy: 0.1112\n",
      "Epoch [4/10], Loss: 16.4991, Accuracy: 0.1039\n",
      "Epoch [5/10], Loss: 15.7448, Accuracy: 0.1023\n",
      "Epoch [6/10], Loss: 14.8864, Accuracy: 0.1000\n",
      "Epoch [7/10], Loss: 13.2210, Accuracy: 0.0992\n",
      "Epoch [8/10], Loss: 25.8806, Accuracy: 0.0989\n",
      "Epoch [9/10], Loss: 19.4008, Accuracy: 0.0986\n",
      "Epoch [10/10], Loss: 7.0968, Accuracy: 0.0987\n"
     ]
    }
   ],
   "source": [
    "train_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "2de987f3-e0bb-42fd-aa65-b038bd166cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Weight Matrices ---\n",
      "layer1.weight:\n",
      "tensor([  1.,  -1.,   1.,   1.,   0., -28.,  -1.,   1.,  -1.,  27.,   1.,  27.,\n",
      "          1.,  -2.,   1.,  -1.,   0., -36.,  27.,  27.,   1.,   0.,  -1.,   0.,\n",
      "         -2.,  27.,  35.,  -2., -28.,   1.,   1.,   0.,   1.,   0.,  -2.,  -2.,\n",
      "          0.,   0.,   0.,   0.,   0.,  27.,   0.,  27.,  -1.,   1.,  -1.,  -1.,\n",
      "          0.,   0.,   0.,   0.,  27.,  -2., -28.,  -2.,   1.,   1.,  27.,   1.,\n",
      "         -2.,   1.,   1.,   1.,   1.,  -2.,   0.,  27.,   0.,   0.,   1.,  -2.,\n",
      "          1.,   1.,   1.,  -1.,  -1.,  -1.,  -1.,   0.,  -1.,  -1.,   0.,  27.,\n",
      "          0.,   1.,   0.,   0.,   1.,   0.,   1.,   0.,  -1.,   1.,   0.,  -2.,\n",
      "          0.,  -1.,  35., -36.,  -2.,   0.,   0.,   1.,   1.,  -1.,  -1.,  27.,\n",
      "          0.,   0., -28.,  -1.,  -1.,  27.,   1., -28.,  -1.,   0.,  -2.,   1.,\n",
      "         -1.,   1.,  -1.,  -1., -28.,   1.,   1.,   0.,  35.,  -2.,   0.,   0.,\n",
      "         -2.,  -2.,  -1.,   1.,   0.,   0.,  27.,  27.,  35.,   1.,  27.,   1.,\n",
      "          1.,   1.,   1.,   0.,   1.,  -2.,   1.,  -1.,  27.,   1.,  -1.,   0.,\n",
      "         -1., -28.,   1.,   0.,  -1.,   0.,   0.,   1.,  27.,   1.,   0.,   0.,\n",
      "         27.,  -1.,  -1.,  27.,   1.,   0.,   0.,  -2.,  -2.,   0.,   1.,  -2.,\n",
      "          1.,  -1.,  27.,  -1.,  35.,   1.,  -1.,  -1.,   1.,   0.,   0.,  -2.,\n",
      "         -1.,  -1.,  -1.,   0.,  -1.,   1.,  -1.,   0.,  27.,  -2.,  27.,  -1.,\n",
      "         27.,  -2.,  35.,   1.,  27.,  27.,   0.,   0.,  27.,  35.,  -1.,   1.,\n",
      "          1.,  -1.,   0.,  -1., -28.,   1.,  27.,   1.,   0.,  -1.,  -2.,  27.,\n",
      "         -1.,  -1.,   0.,   1.,   0.,  -2.,  -1.,   1.,   1.,   1., -28.,   0.,\n",
      "         -1.,   1.,  27.,  -1.,   0.,  -1.,  -1.,   0.,   1.,   0.,   1.,  -1.,\n",
      "         27.,  -2.,  -1.,   0.,   0.,   0.,   0.,  -1.,  -2., -28.,   0.,   1.,\n",
      "         27.,  27.,  -1.,   1.,   1.,   1.,  35., -28.,  27.,   1.,  27.,  -1.,\n",
      "          0., -28.,   0.,   0.,   0.,  -1.,   0.,   0.,  -2.,   0.,   1.,   1.,\n",
      "          1.,   0.,   0., -28.,  27.,  35.,  27.,  35.,  27.,   1.,  -1.,   1.,\n",
      "         -2.,  -1.,   1.,   0.,   0.,   1., -28.,   1.,   1.,  -2.,   0.,  27.,\n",
      "         27.,   0.,   1., -28.,  -1.,  -1.,  -1.,  -1.,   0.,  -1.,   1.,   0.,\n",
      "          1.,   0.,   0.,   1.,   0.,   1.,   0.,  -1.,   0.,  -1.,   0.,  -2.,\n",
      "         -2.,  -1.,   1.,   0.,  27.,   0.,  -1.,  -1.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,  27.,   1.,   1.,  -1.,  -1.,  -2.,   1.,   0.,  27.,  27.,\n",
      "         -1.,   0.,   1.,  -2.,  27.,   0.,   1.,   0.,  -1.,   1.,  -2.,  -2.,\n",
      "         27.,   0.,  27.,  -1.,   1.,   1.,   0.,  27.,  27.,   0.,   1.,   0.,\n",
      "         -1.,   1.,  -1.,   0.,   1.,  -1.,   0.,  35.,  27.,  27.,   0., -28.,\n",
      "         -2.,  -1.,  27.,  -1.,  -1.,  -1.,  27.,  -2.,   0.,  -3.,  -2.,   0.,\n",
      "         27.,  27.,   0.,  27.,   0.,  -1.,  -2.,  35.,  27.,  27.,   0.,  -1.,\n",
      "         -2.,   0.,  -1.,  35.,   0.,   0.,  -1.,   1.,   0.,  -2.,   1.,  -1.,\n",
      "         -1.,  -1.,   0.,   0.,  -1.,  -1.,  27.,  -1.,  -1.,   0.,  -1.,  27.,\n",
      "         -2.,   1.,   1.,   1.,  -2.,   1.,  -2.,   1.,   1.,   1.,  -1.,   0.,\n",
      "         -1.,   1.,   0.,  27., -28.,   1.,  -1.,   0.,   0.,  -2.,  -1.,  -1.,\n",
      "          1.,   0.,   1.,   1.,  -2.,   0.,   0.,   0.,  -1.,  27.,   0.,  -1.,\n",
      "          0.,   0.,  27.,   1.,  35.,  27.,   0.,   0.,   0.,  27.,   0.,  27.,\n",
      "         -2.,   1.,  27.,   1.,  -1.,   0.,   1.,   0.,  -1.,   0.,   0.,   1.,\n",
      "         -1.,   0.,   0.,  -2.,  -1.,  -2.,  -1.,   1.,  27.,  27.,  -1.,  27.,\n",
      "          0.,   1.,   1.,  -1.,   0.,   0.,   1.,   1.,  27.,   0.,  -1., -28.,\n",
      "         -2.,  -1.,   0.,   0.,   1.,   1.,  -1.,   0.,   1.,  27.,  27.,   0.,\n",
      "         -2.,   0.,  -2.,  -2.,  27.,  35.,   1.,   0.,   0.,   1.,   1.,   0.,\n",
      "         -1.,   0.,   1.,   0.,  -1.,  -1.,   0.,  27.,  27.,   0.,  -1.,  -1.,\n",
      "         -2.,  -2.,   0.,   0.,  -1.,  -2.,   0.,   0.,   1.,  -1.,  -2.,   1.,\n",
      "          1.,   0.,  27.,   1.,  -1.,   0.,   0.,   1.,  -1.,   0.,   0.,   1.,\n",
      "          0.,  -1.,  27.,   1.,   0.,   1.,  -2.,  -1.,  -1.,  27.,  -2.,  -2.,\n",
      "          1.,  -1.,  27.,   0.,   1.,   0.,  -2.,   0., -28.,  -1.,   0.,   1.,\n",
      "         -2.,  -1.,  -1.,   0.,   1.,   0.,  27.,   0.,   0.,   1.,  -1.,   1.,\n",
      "          1.,  -1.,  -1.,  27.,   0.,   0.,   0.,  -1.,   0.,   0.,   0.,  -1.,\n",
      "          0.,   1.,  27.,  -2.,   1.,  -2.,   0.,   1.,  35.,   1.,   0.,   1.,\n",
      "         -2.,   1.,   1.,   0.,  35.,  -2.,   1.,   1.,   1.,   0.,   1.,   1.,\n",
      "         -1.,   1.,  27.,  -2.,   1.,  -2.,  -1.,  -2.,  -1.,   0.,  -1.,  46.,\n",
      "         27.,  -2.,  -1.,   0.,   1.,  -1.,   1.,  -2.,   0.,   1.,  -1.,   1.,\n",
      "          1.,  -1.,  -1.,   1.,   1.,  27.,  -1.,   1.,   0.,  -2.,  -1.,  -1.,\n",
      "         35.,  -2.,  27.,   1.,  -2.,  -1.,   1.,  -1.,  -1.,  -1.,  -1.,   0.,\n",
      "          0.,   0.,  -2.,   0.,   1.,  -2.,  -1.,  -1.,   0.,   1.,   0.,  46.,\n",
      "         -1.,   0.,  -2.,  -2.,  -2.,   1.,  -1.,   0.,  27.,   0.,  -2.,   0.,\n",
      "          0.,  -2.,   0.,  -1.,  27.,   0.,  -2.,   0.,  -1.,   0.,   0.,   0.,\n",
      "          0., -28.,   0.,   1.,   1.,   0.,  -1.,   0.,   0.,  27.,  -1.,   1.,\n",
      "          1.,  27.,  -2.,  -1.,   0., -28.,   1.,  -1.,  -1.,   0.,   0.,   0.,\n",
      "          0.,  -2.,   0.,   0.,  -1.,  -2.,  -1.,   0.,  -2.,   0.,   1.,   1.,\n",
      "         -1.,   0.,   0.,  -2.])\n",
      "\n",
      "layer2.weight:\n",
      "tensor([-22.,  -0.,   1.,  27., -22.,   0., -28.,  27.,   1.,  -0.,  -1.,  -1.,\n",
      "         29.,   1., -22.,   1., -22., -22.,  -0.,  -0.,  27.,  -0.,   1.,   1.,\n",
      "         -1.,  -1.,  -1.,   1.,  27.,   1.,  -1.,   0.,  -0.,   1.,  -1.,  27.,\n",
      "         -1., -35.,   1.,  -0.,  -1.,   1.,   1.,  -1.,  -1.,  -0.,   0.,   1.,\n",
      "          1., -22.,  -0.,   0.,  -1.,   1.,   1.,  -1.,   0.,  27.,   0.,   0.,\n",
      "          1.,  29.,   0.,   2.,  27.,  -1.,  27.,  -0.,   1.,  -1.,   0.,   1.,\n",
      "         -0.,   0.,  27.,  -1.,  -1.,   0.,   0.,   0.,   1.,  -0.,   0.,  -1.,\n",
      "          0.,  -0.,   0.,   0., -22.,   0.,   0.,   1.,  22., -22.,   0., -22.,\n",
      "        -22.,  27.,  -0.,   1.,  22.,  -1.,  -1.,  -0.,   1.,  -1.,   1.,  27.,\n",
      "          0., -38.,  -1.,   1.,  -1.,   1.,   0.,  27., -22.,   1.,   1.,  -1.,\n",
      "        -22.,   0.,  22.,   0., -22.,  -1.,  27.,  22.])\n",
      "\n",
      "layer3.weight:\n",
      "tensor([  0.,  -2.,  -2.,   0.,   0.,   0.,  35.,   1.,   0.,   0.,   1.,  -2.,\n",
      "         -1.,   0.,   0.,  -2.,  -1.,   0.,   1.,   0.,   0.,   0.,  -1.,  -1.,\n",
      "         -2.,   0.,   0.,   0.,  -1.,   0.,  -2.,  -1.,   0., -15.,   0.,  -1.,\n",
      "          0.,  -1.,  -1.,   0.,  -1.,  -1.,   0.,  -1.,   1.,   1.,  -2.,  -1.,\n",
      "          0.,  -1.,   1.,   1.,   0.,   0.,   0.,   0.,  -1.,  35.,  35.,   0.,\n",
      "          0.,  -1.,  -2.,  -2.,   0.,   1.,   1.,   0.,  -1.,   0.,   1.,  -2.,\n",
      "          0.,   0., -36.,  27.,   0., -28.,  -1.,  27.,  -1.,   1.,   1.,   0.,\n",
      "          0.,   0.,  -1.,  35.,  -1.,  -2.,   0.,   0.,  -1.,  27.,   0.,  -1.,\n",
      "        -10.,  -1.,  27.,   0.,  -1., -28.,  -1.,   1.,  27.,   0.,   0.,  -1.,\n",
      "          1.,  27.,   1.,   0.,   0.,   1.,  -1.,   0.,   1.,  -1.,   0.,  -1.,\n",
      "        -17.,  -1.,  -1.,  27.,   1.,  -2.,  -2.,   0.])\n",
      "\n",
      "layer4.weight:\n",
      "tensor([ 22., -22.,   1.,   1.,   1., -22.,  22.,  22.,   1.,   0.,  -1.,  -1.,\n",
      "          0.,  -1.,   1.,  29.,   1.,  -1.,  -1.,  -1.,   0.,  -1., -29.,   0.,\n",
      "          1.,   0., -22.,   0.,   0.,  -1.,  -0.,   0.,  -1., -29.,   1.,  22.,\n",
      "          0.,   0.,  -1.,  -1.,  -0.,  23.,   1.,   1.,  -1.,   0.,  -1., -22.,\n",
      "        -22.,  -0.,   0.,   0.,  -1.,   0., -22.,  -1.,   0.,   0.,   0.,  22.,\n",
      "          1.,   0., -22.,  -1.])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x29f5b5840>"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At the end of training, print out all the weight matrices\n",
    "print(\"\\n--- Weight Matrices ---\")\n",
    "for name, param in model.named_parameters():\n",
    "    if \"weight\" in name:\n",
    "        print(f\"{name}:\\n{param.data[0]}\\n\")\n",
    "\n",
    "model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0fad88-ef3f-4a27-bc01-47093a452384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048ad38-c5fa-4358-a559-cdd36ed9dde4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59483c-03fa-49dd-a421-78b2df6ae601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1ac9e884-0dd4-4d4a-ae69-900d93968559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Matrix dimensions\n",
    "dims = {\n",
    "    'matrix1': (784, 64),   # First layer: 784 -> 128\n",
    "    'matrix2': (64, 64),   # Second layer: 128 -> 128\n",
    "    'matrix3': (64, 32),    # Third layer: 128 -> 64\n",
    "    'matrix4': (32, 10)      # Fourth layer: 64 -> 10\n",
    "}\n",
    "\n",
    "def create_weight_matrix(name, shape):\n",
    "    matrix = np.random.randint(-32, 31, size=shape, dtype=np.int64)\n",
    "    with open(f'{name}.mif', 'w') as mif_file:\n",
    "        for row in matrix:\n",
    "            for num in row:\n",
    "                num_32bit = np.int32(num)\n",
    "                unsigned_32bit = num_32bit & 0xFFFFFFFF\n",
    "                mif_file.write(f'{unsigned_32bit:08X}\\n')\n",
    "    return matrix\n",
    "\n",
    "# Generate all matrices\n",
    "matrices = {name: create_weight_matrix(name, shape) \n",
    "           for name, shape in dims.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "52bd7a6c-683f-408b-9e59-45e46efd194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_digit(tensor):\n",
    "    \"\"\"\n",
    "    Visualize a digit represented by a (1, 784) binary tensor.\n",
    "    \n",
    "    Parameters:\n",
    "    tensor (numpy.ndarray): A (1, 784) numpy array representing the binary values of a digit.\n",
    "    \"\"\"\n",
    "    # Reshape the tensor to a 28x28 matrix\n",
    "    digit_image = tensor.reshape(28, 28)\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.imshow(digit_image, cmap='gray')  # 'gray' for black and white images\n",
    "    plt.axis('off')  # Remove axis for a cleaner look\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "18e28a63-efb3-4386-b6d3-63c650137534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def save_random_image(X_train, y_train, idx):\n",
    "    # Randomly select an image index\n",
    "    print(f\"Label: {y_train[idx]}\")\n",
    "    \n",
    "    # Get the corresponding image data (reshape to a 28x28 matrix)\n",
    "    image_data = X_train[idx].numpy()  # Convert to numpy array\n",
    "    image_data = image_data.reshape(28, 28)  # Reshape if needed\n",
    "    \n",
    "    # Open file to save the image data in MIF format\n",
    "    with open(f'image.mif', 'w') as file:\n",
    "        # Iterate through each pixel and write it as 32-bit unsigned hex\n",
    "        for i in range(28):\n",
    "            for j in range(28):\n",
    "                pixel = image_data[i, j]\n",
    "                # Convert pixel to integer (0 or 1)\n",
    "                int_pixel = int(pixel)  \n",
    "                # Convert the integer to a 32-bit unsigned number (mask to ensure 32-bit)\n",
    "                unsigned_32bit = np.int32(int_pixel) & 0xFFFFFFFF\n",
    "                # Write the 32-bit unsigned value in hex\n",
    "                file.write(f'{unsigned_32bit:08X}\\n')\n",
    "\n",
    "# Example usage (assuming X_train and y_train are already defined):\n",
    "save_random_image(X_train, y_train, sample_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9a1a8c85-444d-4dd0-aaf1-719411a3d61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 784)\n",
      "After Layer 1 (x1) shape: (1, 64)\n",
      "After Layer 1:\n",
      "[[-142   88 -225 -202 -203 -125  204 -205   60 -129 -266 -295   70  -75\n",
      "   -14  199  204  -37 -314   74   -3  -80  -47  -58  287 -280  -10   34\n",
      "  -263  154   15 -366   20  -74 -172 -215 -219 -117 -178 -100 -122 -274\n",
      "   -95 -127   99 -367  206  196 -187 -160 -350 -145 -157  -48  -85 -148\n",
      "    30 -115 -157 -245   20 -396 -295   11]]\n",
      "After ReLU (x1) shape: (1, 64)\n",
      "After ReLU (x1):\n",
      "[[  0  88   0   0   0   0 204   0  60   0   0   0  70   0   0 199 204   0\n",
      "    0  74   0   0   0   0 287   0   0  34   0 154  15   0  20   0   0   0\n",
      "    0   0   0   0   0   0   0   0  99   0 206 196   0   0   0   0   0   0\n",
      "    0   0  30   0   0   0  20   0   0  11]]\n",
      "After Layer 2 (x2) shape: (1, 64)\n",
      "After Layer 2:\n",
      "[[  7527   1595 -14981  -1680   3770  13914   1898  -1957   8310  -4052\n",
      "    1475  16045 -12030  13079 -16870   9688  17176   4034  -9999   3242\n",
      "  -10207 -10868   5467  -6824  -9165   3182  -1823   8461  -4785  -5101\n",
      "    -383  -4084   -166  -5656   -274  18022   5879   8925  -4174   2198\n",
      "    -905   7957 -12826  -3770   7813  -5841   -650  -2756  -3768  -7255\n",
      "  -23388 -21044   3268   -151   1758 -10803 -15347   4692 -10907  -4879\n",
      "  -25755   9115  -7323   8171]]\n",
      "After ReLU (x2) shape: (1, 64)\n",
      "After ReLU (x2):\n",
      "[[ 7527  1595     0     0  3770 13914  1898     0  8310     0  1475 16045\n",
      "      0 13079     0  9688 17176  4034     0  3242     0     0  5467     0\n",
      "      0  3182     0  8461     0     0     0     0     0     0     0 18022\n",
      "   5879  8925     0  2198     0  7957     0     0  7813     0     0     0\n",
      "      0     0     0     0  3268     0  1758     0     0  4692     0     0\n",
      "      0  9115     0  8171]]\n",
      "After Layer 3 (x3) shape: (1, 32)\n",
      "After Layer 3:\n",
      "[[ -259374 -1218835  -468091   167763   -82267   292006 -1015997 -1291787\n",
      "   -767295   455897  -830686  -503960  -413032  -238688  -382420  -502047\n",
      "     99870  -157380   129162 -2208478  1136272   936780    48799   507445\n",
      "   -642253 -1236979   727935   360773   -92368   418416  -630220  -175961]]\n",
      "After ReLU (x3) shape: (1, 32)\n",
      "After ReLU (x3):\n",
      "[[      0       0       0  167763       0  292006       0       0       0\n",
      "   455897       0       0       0       0       0       0   99870       0\n",
      "   129162       0 1136272  936780   48799  507445       0       0  727935\n",
      "   360773       0  418416       0       0]]\n",
      "After Layer 4 (x4) shape: (1, 10)\n",
      "After Layer 4:\n",
      "[[    69946  34516970   2422668  51139584 -22800079   2606677  -3240038\n",
      "   52843055  50381737 -19324497]]\n",
      "Output shape: (1, 10)\n",
      "\n",
      "Output logits:\n",
      "[[    69946  34516970   2422668  51139584 -22800079   2606677  -3240038\n",
      "   52843055  50381737 -19324497]]\n",
      "\n",
      "Predicted class: 7\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train, matrices, and output are defined\n",
    "sample_idx = 400\n",
    "sample = X_train[sample_idx:sample_idx+1]  # Select the 6th sample\n",
    "\n",
    "x = sample\n",
    "visualize_digit(x)\n",
    "\n",
    "# Convert tensor to numpy and cast to int64 for matrix multiplication\n",
    "x = x.numpy().astype(np.int64)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Layer 1: (1,784) @ (784,128) -> (1,128)\n",
    "x1 = x @ matrices['matrix1']  # Matrix multiplication\n",
    "print(f\"After Layer 1 (x1) shape: {x1.shape}\")\n",
    "print(\"After Layer 1:\")\n",
    "print(x1)\n",
    "\n",
    "# ReLU activation\n",
    "x1 = np.maximum(0, x1)  # ReLU\n",
    "print(f\"After ReLU (x1) shape: {x1.shape}\")\n",
    "print(\"After ReLU (x1):\")\n",
    "print(x1)\n",
    "\n",
    "# Layer 2: (1,128) @ (128,128) -> (1,128)\n",
    "x2 = x1 @ matrices['matrix2']  # Matrix multiplication\n",
    "print(f\"After Layer 2 (x2) shape: {x2.shape}\")\n",
    "print(\"After Layer 2:\")\n",
    "print(x2)\n",
    "\n",
    "# ReLU activation\n",
    "x2 = np.maximum(0, x2)  # ReLU\n",
    "print(f\"After ReLU (x2) shape: {x2.shape}\")\n",
    "print(\"After ReLU (x2):\")\n",
    "print(x2)\n",
    "\n",
    "# Layer 3: (1,128) @ (128,64) -> (1,64)\n",
    "x3 = x2 @ matrices['matrix3']  # Matrix multiplication\n",
    "print(f\"After Layer 3 (x3) shape: {x3.shape}\")\n",
    "print(\"After Layer 3:\")\n",
    "print(x3)\n",
    "\n",
    "# ReLU activation\n",
    "x3 = np.maximum(0, x3)  # ReLU\n",
    "print(f\"After ReLU (x3) shape: {x3.shape}\")\n",
    "print(\"After ReLU (x3):\")\n",
    "print(x3)\n",
    "\n",
    "# Layer 4: (1,64) @ (64,10) -> (1,10)\n",
    "x4 = x3 @ matrices['matrix4']  # Matrix multiplication\n",
    "print(f\"After Layer 4 (x4) shape: {x4.shape}\")\n",
    "print(\"After Layer 4:\")\n",
    "print(x4)\n",
    "\n",
    "# Output logits\n",
    "output = x4\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"\\nOutput logits:\")\n",
    "print(output)\n",
    "\n",
    "# Predicted class\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"\\nPredicted class:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
