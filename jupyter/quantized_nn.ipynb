{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b02af22-735d-48ee-8f18-2a955e216161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class IntegerLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # Initialize weights as floating-point values\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features, dtype=torch.float32))  # Use normal distribution for initialization\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.mm(x, self.weight.t())\n",
    "    \n",
    "    def quantize_weights(self, target_min=--32, target_max=31):\n",
    "        \"\"\"Quantize the weights to a target range with dynamic scaling.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Find the min and max of the weights\n",
    "            weight_min = self.weight.min()\n",
    "            weight_max = self.weight.max()\n",
    "            \n",
    "            # Dynamically scale the target range based on observed weight distribution\n",
    "            scale_factor = 0.3 # Adjust this factor to control how aggressively the weights are scaled\n",
    "            range_span = (weight_max - weight_min) * scale_factor\n",
    "            target_min_scaled = weight_min - range_span / 2\n",
    "            target_max_scaled = weight_max + range_span / 2\n",
    "            \n",
    "            # Scale the weights to the scaled target range\n",
    "            scale = (target_max_scaled - target_min_scaled) / (weight_max - weight_min)\n",
    "            zero_point = target_min_scaled - weight_min * scale\n",
    "            \n",
    "            # Quantize the weights\n",
    "            quantized_weights = torch.round(self.weight * scale + zero_point)\n",
    "            \n",
    "            # Clip the values to ensure they are within the target range\n",
    "            quantized_weights = torch.clamp(quantized_weights, target_min, target_max)\n",
    "            \n",
    "            # Store the quantized weights back into the model\n",
    "            self.weight.data = quantized_weights\n",
    "    \n",
    "class IntegerNet(nn.Module):\n",
    "    def __init__(self, input_size=784):\n",
    "        super().__init__()\n",
    "        # Four layers of IntegerLinear\n",
    "        self.layer1 = IntegerLinear(input_size, 128)\n",
    "        self.layer2 = IntegerLinear(128, 128)\n",
    "        self.layer3 = IntegerLinear(128, 64)\n",
    "        self.layer4 = IntegerLinear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def quantize_weights(self):\n",
    "        \"\"\"Quantize weights for all layers.\"\"\"\n",
    "        self.layer1.quantize_weights()\n",
    "        self.layer2.quantize_weights()\n",
    "        self.layer3.quantize_weights()\n",
    "        self.layer4.quantize_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbac7b07-ebf2-4ad4-934a-e989818ce91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath='train.csv'):\n",
    "    data = pd.read_csv(filepath)\n",
    "    labels = data['label'].values\n",
    "    pixels = data.drop('label', axis=1).values\n",
    "    \n",
    "    # Convert to binary (0 or 1)\n",
    "    pixels = (pixels > 127).astype(np.float32)\n",
    "    \n",
    "    return torch.FloatTensor(pixels), torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e1971-b174-4b38-a7ef-4324724a1019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc5a6eb-63a3-46b9-9cdf-57ead970f28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, epochs=10, batch_size=128):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "    \n",
    "    n_samples = X_train.shape[0]\n",
    "    n_batches = n_samples // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        \n",
    "        # Shuffle data\n",
    "        indices = torch.randperm(n_samples)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            batch_X = X_train[start_idx:end_idx]\n",
    "            batch_y = y_train[start_idx:end_idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        avg_loss = total_loss / n_batches\n",
    "        accuracy = correct / n_samples\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "        \n",
    "        # Quantize weights after each epoch to reduce their size\n",
    "        model.quantize_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c4a61-290e-4e9a-9a63-671eaff6df4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7500d0-6ba6-4ed2-9d95-bccceba6b928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d1378-edb0-4d73-bdf4-29ca1fd3df02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb20667a-dffa-474b-aa08-be43c7449abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_data()\n",
    "model = IntegerNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "169c555d-485b-4d5a-9570-6f95b1692e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42000, 784])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "03c0c3f3-6fc6-475b-a403-cfd0c19c0a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 236.2272, Accuracy: 0.3083\n",
      "Epoch [2/10], Loss: 11.6330, Accuracy: 0.1227\n",
      "Epoch [3/10], Loss: 14.5360, Accuracy: 0.1112\n",
      "Epoch [4/10], Loss: 16.4991, Accuracy: 0.1039\n",
      "Epoch [5/10], Loss: 15.7448, Accuracy: 0.1023\n",
      "Epoch [6/10], Loss: 14.8864, Accuracy: 0.1000\n",
      "Epoch [7/10], Loss: 13.2210, Accuracy: 0.0992\n",
      "Epoch [8/10], Loss: 25.8806, Accuracy: 0.0989\n",
      "Epoch [9/10], Loss: 19.4008, Accuracy: 0.0986\n",
      "Epoch [10/10], Loss: 7.0968, Accuracy: 0.0987\n"
     ]
    }
   ],
   "source": [
    "train_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "2de987f3-e0bb-42fd-aa65-b038bd166cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Weight Matrices ---\n",
      "layer1.weight:\n",
      "tensor([  1.,  -1.,   1.,   1.,   0., -28.,  -1.,   1.,  -1.,  27.,   1.,  27.,\n",
      "          1.,  -2.,   1.,  -1.,   0., -36.,  27.,  27.,   1.,   0.,  -1.,   0.,\n",
      "         -2.,  27.,  35.,  -2., -28.,   1.,   1.,   0.,   1.,   0.,  -2.,  -2.,\n",
      "          0.,   0.,   0.,   0.,   0.,  27.,   0.,  27.,  -1.,   1.,  -1.,  -1.,\n",
      "          0.,   0.,   0.,   0.,  27.,  -2., -28.,  -2.,   1.,   1.,  27.,   1.,\n",
      "         -2.,   1.,   1.,   1.,   1.,  -2.,   0.,  27.,   0.,   0.,   1.,  -2.,\n",
      "          1.,   1.,   1.,  -1.,  -1.,  -1.,  -1.,   0.,  -1.,  -1.,   0.,  27.,\n",
      "          0.,   1.,   0.,   0.,   1.,   0.,   1.,   0.,  -1.,   1.,   0.,  -2.,\n",
      "          0.,  -1.,  35., -36.,  -2.,   0.,   0.,   1.,   1.,  -1.,  -1.,  27.,\n",
      "          0.,   0., -28.,  -1.,  -1.,  27.,   1., -28.,  -1.,   0.,  -2.,   1.,\n",
      "         -1.,   1.,  -1.,  -1., -28.,   1.,   1.,   0.,  35.,  -2.,   0.,   0.,\n",
      "         -2.,  -2.,  -1.,   1.,   0.,   0.,  27.,  27.,  35.,   1.,  27.,   1.,\n",
      "          1.,   1.,   1.,   0.,   1.,  -2.,   1.,  -1.,  27.,   1.,  -1.,   0.,\n",
      "         -1., -28.,   1.,   0.,  -1.,   0.,   0.,   1.,  27.,   1.,   0.,   0.,\n",
      "         27.,  -1.,  -1.,  27.,   1.,   0.,   0.,  -2.,  -2.,   0.,   1.,  -2.,\n",
      "          1.,  -1.,  27.,  -1.,  35.,   1.,  -1.,  -1.,   1.,   0.,   0.,  -2.,\n",
      "         -1.,  -1.,  -1.,   0.,  -1.,   1.,  -1.,   0.,  27.,  -2.,  27.,  -1.,\n",
      "         27.,  -2.,  35.,   1.,  27.,  27.,   0.,   0.,  27.,  35.,  -1.,   1.,\n",
      "          1.,  -1.,   0.,  -1., -28.,   1.,  27.,   1.,   0.,  -1.,  -2.,  27.,\n",
      "         -1.,  -1.,   0.,   1.,   0.,  -2.,  -1.,   1.,   1.,   1., -28.,   0.,\n",
      "         -1.,   1.,  27.,  -1.,   0.,  -1.,  -1.,   0.,   1.,   0.,   1.,  -1.,\n",
      "         27.,  -2.,  -1.,   0.,   0.,   0.,   0.,  -1.,  -2., -28.,   0.,   1.,\n",
      "         27.,  27.,  -1.,   1.,   1.,   1.,  35., -28.,  27.,   1.,  27.,  -1.,\n",
      "          0., -28.,   0.,   0.,   0.,  -1.,   0.,   0.,  -2.,   0.,   1.,   1.,\n",
      "          1.,   0.,   0., -28.,  27.,  35.,  27.,  35.,  27.,   1.,  -1.,   1.,\n",
      "         -2.,  -1.,   1.,   0.,   0.,   1., -28.,   1.,   1.,  -2.,   0.,  27.,\n",
      "         27.,   0.,   1., -28.,  -1.,  -1.,  -1.,  -1.,   0.,  -1.,   1.,   0.,\n",
      "          1.,   0.,   0.,   1.,   0.,   1.,   0.,  -1.,   0.,  -1.,   0.,  -2.,\n",
      "         -2.,  -1.,   1.,   0.,  27.,   0.,  -1.,  -1.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,  27.,   1.,   1.,  -1.,  -1.,  -2.,   1.,   0.,  27.,  27.,\n",
      "         -1.,   0.,   1.,  -2.,  27.,   0.,   1.,   0.,  -1.,   1.,  -2.,  -2.,\n",
      "         27.,   0.,  27.,  -1.,   1.,   1.,   0.,  27.,  27.,   0.,   1.,   0.,\n",
      "         -1.,   1.,  -1.,   0.,   1.,  -1.,   0.,  35.,  27.,  27.,   0., -28.,\n",
      "         -2.,  -1.,  27.,  -1.,  -1.,  -1.,  27.,  -2.,   0.,  -3.,  -2.,   0.,\n",
      "         27.,  27.,   0.,  27.,   0.,  -1.,  -2.,  35.,  27.,  27.,   0.,  -1.,\n",
      "         -2.,   0.,  -1.,  35.,   0.,   0.,  -1.,   1.,   0.,  -2.,   1.,  -1.,\n",
      "         -1.,  -1.,   0.,   0.,  -1.,  -1.,  27.,  -1.,  -1.,   0.,  -1.,  27.,\n",
      "         -2.,   1.,   1.,   1.,  -2.,   1.,  -2.,   1.,   1.,   1.,  -1.,   0.,\n",
      "         -1.,   1.,   0.,  27., -28.,   1.,  -1.,   0.,   0.,  -2.,  -1.,  -1.,\n",
      "          1.,   0.,   1.,   1.,  -2.,   0.,   0.,   0.,  -1.,  27.,   0.,  -1.,\n",
      "          0.,   0.,  27.,   1.,  35.,  27.,   0.,   0.,   0.,  27.,   0.,  27.,\n",
      "         -2.,   1.,  27.,   1.,  -1.,   0.,   1.,   0.,  -1.,   0.,   0.,   1.,\n",
      "         -1.,   0.,   0.,  -2.,  -1.,  -2.,  -1.,   1.,  27.,  27.,  -1.,  27.,\n",
      "          0.,   1.,   1.,  -1.,   0.,   0.,   1.,   1.,  27.,   0.,  -1., -28.,\n",
      "         -2.,  -1.,   0.,   0.,   1.,   1.,  -1.,   0.,   1.,  27.,  27.,   0.,\n",
      "         -2.,   0.,  -2.,  -2.,  27.,  35.,   1.,   0.,   0.,   1.,   1.,   0.,\n",
      "         -1.,   0.,   1.,   0.,  -1.,  -1.,   0.,  27.,  27.,   0.,  -1.,  -1.,\n",
      "         -2.,  -2.,   0.,   0.,  -1.,  -2.,   0.,   0.,   1.,  -1.,  -2.,   1.,\n",
      "          1.,   0.,  27.,   1.,  -1.,   0.,   0.,   1.,  -1.,   0.,   0.,   1.,\n",
      "          0.,  -1.,  27.,   1.,   0.,   1.,  -2.,  -1.,  -1.,  27.,  -2.,  -2.,\n",
      "          1.,  -1.,  27.,   0.,   1.,   0.,  -2.,   0., -28.,  -1.,   0.,   1.,\n",
      "         -2.,  -1.,  -1.,   0.,   1.,   0.,  27.,   0.,   0.,   1.,  -1.,   1.,\n",
      "          1.,  -1.,  -1.,  27.,   0.,   0.,   0.,  -1.,   0.,   0.,   0.,  -1.,\n",
      "          0.,   1.,  27.,  -2.,   1.,  -2.,   0.,   1.,  35.,   1.,   0.,   1.,\n",
      "         -2.,   1.,   1.,   0.,  35.,  -2.,   1.,   1.,   1.,   0.,   1.,   1.,\n",
      "         -1.,   1.,  27.,  -2.,   1.,  -2.,  -1.,  -2.,  -1.,   0.,  -1.,  46.,\n",
      "         27.,  -2.,  -1.,   0.,   1.,  -1.,   1.,  -2.,   0.,   1.,  -1.,   1.,\n",
      "          1.,  -1.,  -1.,   1.,   1.,  27.,  -1.,   1.,   0.,  -2.,  -1.,  -1.,\n",
      "         35.,  -2.,  27.,   1.,  -2.,  -1.,   1.,  -1.,  -1.,  -1.,  -1.,   0.,\n",
      "          0.,   0.,  -2.,   0.,   1.,  -2.,  -1.,  -1.,   0.,   1.,   0.,  46.,\n",
      "         -1.,   0.,  -2.,  -2.,  -2.,   1.,  -1.,   0.,  27.,   0.,  -2.,   0.,\n",
      "          0.,  -2.,   0.,  -1.,  27.,   0.,  -2.,   0.,  -1.,   0.,   0.,   0.,\n",
      "          0., -28.,   0.,   1.,   1.,   0.,  -1.,   0.,   0.,  27.,  -1.,   1.,\n",
      "          1.,  27.,  -2.,  -1.,   0., -28.,   1.,  -1.,  -1.,   0.,   0.,   0.,\n",
      "          0.,  -2.,   0.,   0.,  -1.,  -2.,  -1.,   0.,  -2.,   0.,   1.,   1.,\n",
      "         -1.,   0.,   0.,  -2.])\n",
      "\n",
      "layer2.weight:\n",
      "tensor([-22.,  -0.,   1.,  27., -22.,   0., -28.,  27.,   1.,  -0.,  -1.,  -1.,\n",
      "         29.,   1., -22.,   1., -22., -22.,  -0.,  -0.,  27.,  -0.,   1.,   1.,\n",
      "         -1.,  -1.,  -1.,   1.,  27.,   1.,  -1.,   0.,  -0.,   1.,  -1.,  27.,\n",
      "         -1., -35.,   1.,  -0.,  -1.,   1.,   1.,  -1.,  -1.,  -0.,   0.,   1.,\n",
      "          1., -22.,  -0.,   0.,  -1.,   1.,   1.,  -1.,   0.,  27.,   0.,   0.,\n",
      "          1.,  29.,   0.,   2.,  27.,  -1.,  27.,  -0.,   1.,  -1.,   0.,   1.,\n",
      "         -0.,   0.,  27.,  -1.,  -1.,   0.,   0.,   0.,   1.,  -0.,   0.,  -1.,\n",
      "          0.,  -0.,   0.,   0., -22.,   0.,   0.,   1.,  22., -22.,   0., -22.,\n",
      "        -22.,  27.,  -0.,   1.,  22.,  -1.,  -1.,  -0.,   1.,  -1.,   1.,  27.,\n",
      "          0., -38.,  -1.,   1.,  -1.,   1.,   0.,  27., -22.,   1.,   1.,  -1.,\n",
      "        -22.,   0.,  22.,   0., -22.,  -1.,  27.,  22.])\n",
      "\n",
      "layer3.weight:\n",
      "tensor([  0.,  -2.,  -2.,   0.,   0.,   0.,  35.,   1.,   0.,   0.,   1.,  -2.,\n",
      "         -1.,   0.,   0.,  -2.,  -1.,   0.,   1.,   0.,   0.,   0.,  -1.,  -1.,\n",
      "         -2.,   0.,   0.,   0.,  -1.,   0.,  -2.,  -1.,   0., -15.,   0.,  -1.,\n",
      "          0.,  -1.,  -1.,   0.,  -1.,  -1.,   0.,  -1.,   1.,   1.,  -2.,  -1.,\n",
      "          0.,  -1.,   1.,   1.,   0.,   0.,   0.,   0.,  -1.,  35.,  35.,   0.,\n",
      "          0.,  -1.,  -2.,  -2.,   0.,   1.,   1.,   0.,  -1.,   0.,   1.,  -2.,\n",
      "          0.,   0., -36.,  27.,   0., -28.,  -1.,  27.,  -1.,   1.,   1.,   0.,\n",
      "          0.,   0.,  -1.,  35.,  -1.,  -2.,   0.,   0.,  -1.,  27.,   0.,  -1.,\n",
      "        -10.,  -1.,  27.,   0.,  -1., -28.,  -1.,   1.,  27.,   0.,   0.,  -1.,\n",
      "          1.,  27.,   1.,   0.,   0.,   1.,  -1.,   0.,   1.,  -1.,   0.,  -1.,\n",
      "        -17.,  -1.,  -1.,  27.,   1.,  -2.,  -2.,   0.])\n",
      "\n",
      "layer4.weight:\n",
      "tensor([ 22., -22.,   1.,   1.,   1., -22.,  22.,  22.,   1.,   0.,  -1.,  -1.,\n",
      "          0.,  -1.,   1.,  29.,   1.,  -1.,  -1.,  -1.,   0.,  -1., -29.,   0.,\n",
      "          1.,   0., -22.,   0.,   0.,  -1.,  -0.,   0.,  -1., -29.,   1.,  22.,\n",
      "          0.,   0.,  -1.,  -1.,  -0.,  23.,   1.,   1.,  -1.,   0.,  -1., -22.,\n",
      "        -22.,  -0.,   0.,   0.,  -1.,   0., -22.,  -1.,   0.,   0.,   0.,  22.,\n",
      "          1.,   0., -22.,  -1.])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x29f5b5840>"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At the end of training, print out all the weight matrices\n",
    "print(\"\\n--- Weight Matrices ---\")\n",
    "for name, param in model.named_parameters():\n",
    "    if \"weight\" in name:\n",
    "        print(f\"{name}:\\n{param.data[0]}\\n\")\n",
    "\n",
    "model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0fad88-ef3f-4a27-bc01-47093a452384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048ad38-c5fa-4358-a559-cdd36ed9dde4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59483c-03fa-49dd-a421-78b2df6ae601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1ac9e884-0dd4-4d4a-ae69-900d93968559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Matrix dimensions\n",
    "dims = {\n",
    "    'matrix1': (784, 64),   # First layer: 784 -> 128\n",
    "    'matrix2': (64, 64),   # Second layer: 128 -> 128\n",
    "    'matrix3': (64, 32),    # Third layer: 128 -> 64\n",
    "    'matrix4': (32, 10)      # Fourth layer: 64 -> 10\n",
    "}\n",
    "\n",
    "def create_weight_matrix(name, shape):\n",
    "    matrix = np.random.randint(-32, 31, size=shape, dtype=np.int64)\n",
    "    with open(f'{name}.mif', 'w') as mif_file:\n",
    "        for row in matrix:\n",
    "            for num in row:\n",
    "                num_32bit = np.int32(num)\n",
    "                unsigned_32bit = num_32bit & 0xFFFFFFFF\n",
    "                mif_file.write(f'{unsigned_32bit:08X}\\n')\n",
    "    return matrix\n",
    "\n",
    "# Generate all matrices\n",
    "matrices = {name: create_weight_matrix(name, shape) \n",
    "           for name, shape in dims.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "52bd7a6c-683f-408b-9e59-45e46efd194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_digit(tensor):\n",
    "    \"\"\"\n",
    "    Visualize a digit represented by a (1, 784) binary tensor.\n",
    "    \n",
    "    Parameters:\n",
    "    tensor (numpy.ndarray): A (1, 784) numpy array representing the binary values of a digit.\n",
    "    \"\"\"\n",
    "    # Reshape the tensor to a 28x28 matrix\n",
    "    digit_image = tensor.reshape(28, 28)\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.imshow(digit_image, cmap='gray')  # 'gray' for black and white images\n",
    "    plt.axis('off')  # Remove axis for a cleaner look\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "18e28a63-efb3-4386-b6d3-63c650137534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def save_random_image(X_train, y_train, idx):\n",
    "    # Randomly select an image index\n",
    "    print(f\"Label: {y_train[idx]}\")\n",
    "    \n",
    "    # Get the corresponding image data (reshape to a 28x28 matrix)\n",
    "    image_data = X_train[idx].numpy()  # Convert to numpy array\n",
    "    image_data = image_data.reshape(28, 28)  # Reshape if needed\n",
    "    \n",
    "    # Open file to save the image data in MIF format\n",
    "    with open(f'image.mif', 'w') as file:\n",
    "        # Iterate through each pixel and write it as 32-bit unsigned hex\n",
    "        for i in range(28):\n",
    "            for j in range(28):\n",
    "                pixel = image_data[i, j]\n",
    "                # Convert pixel to integer (0 or 1)\n",
    "                int_pixel = int(pixel)  \n",
    "                # Convert the integer to a 32-bit unsigned number (mask to ensure 32-bit)\n",
    "                unsigned_32bit = np.int32(int_pixel) & 0xFFFFFFFF\n",
    "                # Write the 32-bit unsigned value in hex\n",
    "                file.write(f'{unsigned_32bit:08X}\\n')\n",
    "\n",
    "# Example usage (assuming X_train and y_train are already defined):\n",
    "save_random_image(X_train, y_train, sample_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9a1a8c85-444d-4dd0-aaf1-719411a3d61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 784)\n",
      "After Layer 1 (x1) shape: (1, 64)\n",
      "After Layer 1:\n",
      "[[ -29 -107 -305 -198    5  -21 -132   92  -95   49 -194 -212  234   16\n",
      "   -72  -72 -219 -180 -201 -134 -171 -246   59 -136  -55 -151 -104   12\n",
      "  -296 -141 -110   36 -120  132  -55 -263   49 -174 -271   14  -83 -136\n",
      "   187 -152  -76 -290  108  -75 -251 -232   16  -57 -216   41  -12   25\n",
      "    91 -177   -7 -236   63 -141  -70  200]]\n",
      "After ReLU (x1) shape: (1, 64)\n",
      "After ReLU (x1):\n",
      "[[  0   0   0   0   5   0   0  92   0  49   0   0 234  16   0   0   0   0\n",
      "    0   0   0   0  59   0   0   0   0  12   0   0   0  36   0 132   0   0\n",
      "   49   0   0  14   0   0 187   0   0   0 108   0   0   0  16   0   0  41\n",
      "    0  25  91   0   0   0  63   0   0 200]]\n",
      "After Layer 2 (x2) shape: (1, 64)\n",
      "After Layer 2:\n",
      "[[ 10068   2366  -2332   2969   4851 -13666   -515 -15164  -5899   4627\n",
      "   -3762 -11132   5059  -8085  11019 -14361   3978  -6926 -19086   7982\n",
      "   -6912 -10678  -7828 -10080 -14938    571   5751   4554   6929   5842\n",
      "  -15667  -1977  -2691  -8484   1760   1753   4295   3912   5407  -7655\n",
      "   -1465  -7472  -2835  -5930 -10033 -12662  -6707   1205   8266   2692\n",
      "   -5641  10011   2977   6321 -23468  -6333  -2384   1280  -2288 -10218\n",
      "   -4208  -3670   4532  -9465]]\n",
      "After ReLU (x2) shape: (1, 64)\n",
      "After ReLU (x2):\n",
      "[[10068  2366     0  2969  4851     0     0     0     0  4627     0     0\n",
      "   5059     0 11019     0  3978     0     0  7982     0     0     0     0\n",
      "      0   571  5751  4554  6929  5842     0     0     0     0  1760  1753\n",
      "   4295  3912  5407     0     0     0     0     0     0     0     0  1205\n",
      "   8266  2692     0 10011  2977  6321     0     0     0  1280     0     0\n",
      "      0     0  4532     0]]\n",
      "After Layer 3 (x3) shape: (1, 32)\n",
      "After Layer 3:\n",
      "[[   18698  -381826  -622154   128932    -8738   314458  -528754  -490338\n",
      "   -591679   719495   250424   351905  -593852  -184176   474466 -1410512\n",
      "    -61395   -93337  -221778  -502913   309344   385737  -131591  -601197\n",
      "   -387512  -366304   -68759   118818  -530195   380955  -984503   448914]]\n",
      "After ReLU (x3) shape: (1, 32)\n",
      "After ReLU (x3):\n",
      "[[ 18698      0      0 128932      0 314458      0      0      0 719495\n",
      "  250424 351905      0      0 474466      0      0      0      0      0\n",
      "  309344 385737      0      0      0      0      0 118818      0 380955\n",
      "       0 448914]]\n",
      "After Layer 4 (x4) shape: (1, 10)\n",
      "After Layer 4:\n",
      "[[ 45099171  -1698080  19067690   5973151  -7891207  19870939   3410388\n",
      "   40427752  54056148 -39845126]]\n",
      "Output shape: (1, 10)\n",
      "\n",
      "Output logits:\n",
      "[[ 45099171  -1698080  19067690   5973151  -7891207  19870939   3410388\n",
      "   40427752  54056148 -39845126]]\n",
      "\n",
      "Predicted class: 8\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train, matrices, and output are defined\n",
    "sample_idx = 6\n",
    "sample = X_train[sample_idx:sample_idx+1]  # Select the 6th sample\n",
    "\n",
    "x = sample\n",
    "visualize_digit(x)\n",
    "\n",
    "# Convert tensor to numpy and cast to int64 for matrix multiplication\n",
    "x = x.numpy().astype(np.int64)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Layer 1: (1,784) @ (784,128) -> (1,128)\n",
    "x1 = x @ matrices['matrix1']  # Matrix multiplication\n",
    "print(f\"After Layer 1 (x1) shape: {x1.shape}\")\n",
    "print(\"After Layer 1:\")\n",
    "print(x1)\n",
    "\n",
    "# ReLU activation\n",
    "x1 = np.maximum(0, x1)  # ReLU\n",
    "print(f\"After ReLU (x1) shape: {x1.shape}\")\n",
    "print(\"After ReLU (x1):\")\n",
    "print(x1)\n",
    "\n",
    "# Layer 2: (1,128) @ (128,128) -> (1,128)\n",
    "x2 = x1 @ matrices['matrix2']  # Matrix multiplication\n",
    "print(f\"After Layer 2 (x2) shape: {x2.shape}\")\n",
    "print(\"After Layer 2:\")\n",
    "print(x2)\n",
    "\n",
    "# ReLU activation\n",
    "x2 = np.maximum(0, x2)  # ReLU\n",
    "print(f\"After ReLU (x2) shape: {x2.shape}\")\n",
    "print(\"After ReLU (x2):\")\n",
    "print(x2)\n",
    "\n",
    "# Layer 3: (1,128) @ (128,64) -> (1,64)\n",
    "x3 = x2 @ matrices['matrix3']  # Matrix multiplication\n",
    "print(f\"After Layer 3 (x3) shape: {x3.shape}\")\n",
    "print(\"After Layer 3:\")\n",
    "print(x3)\n",
    "\n",
    "# ReLU activation\n",
    "x3 = np.maximum(0, x3)  # ReLU\n",
    "print(f\"After ReLU (x3) shape: {x3.shape}\")\n",
    "print(\"After ReLU (x3):\")\n",
    "print(x3)\n",
    "\n",
    "# Layer 4: (1,64) @ (64,10) -> (1,10)\n",
    "x4 = x3 @ matrices['matrix4']  # Matrix multiplication\n",
    "print(f\"After Layer 4 (x4) shape: {x4.shape}\")\n",
    "print(\"After Layer 4:\")\n",
    "print(x4)\n",
    "\n",
    "# Output logits\n",
    "output = x4\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"\\nOutput logits:\")\n",
    "print(output)\n",
    "\n",
    "# Predicted class\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"\\nPredicted class:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
